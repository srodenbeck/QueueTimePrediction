{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2d27d-48bf-4e4f-900e-bdb52d31c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in dataframes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from tqdm import tqdm\n",
    "from intervaltree import IntervalTree\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import config_file\n",
    "import read_db\n",
    "\n",
    "all_df = None\n",
    "\n",
    "def calculate_running_features(engine):\n",
    "    \"\"\"\n",
    "    calculate_running_features()\n",
    "\n",
    "    Calculates features relating to jobs currently running when another job is\n",
    "    made eligible. Features include number of jobs running, the total number of\n",
    "    cpus in use by running jobs, the total amount of memory being used by\n",
    "    running jobs, the total amount of nodes being used by running jobs, and the\n",
    "    combined timelimit for all running jobs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    engine : SQLALCHEMY ENGINE\n",
    "        Instance of sqlalchemy engine to access postgres database.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    # Read in dataframe\n",
    "    all_df = pd.read_sql_query(\"SELECT * FROM jobs_queue_only ORDER BY eligible\", engine)\n",
    "    df = pd.read_sql_query(\n",
    "        \"SELECT job_id, eligible, start_time, end_time, req_cpus, req_mem, req_nodes, time_limit_raw, priority FROM jobs_queue_only ORDER BY eligible\",\n",
    "        engine)\n",
    "    df['start_time'] = df['start_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['end_time'] = df['end_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['eligible'] = df['eligible'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['start_time'] = df['start_time'] - 1631807119\n",
    "    df['end_time'] = df['end_time'] - 1631807119\n",
    "    df['eligible'] = df['eligible'] - 1631807119\n",
    "    np_array = df.to_numpy()\n",
    "\n",
    "    # Dict to be used for indexing columns of np_array.\n",
    "    idx_dict = {\n",
    "        \"JOB_ID\": 0,\n",
    "        \"ELIGIBLE\": 1,\n",
    "        \"START_TIME\": 2,\n",
    "        \"END_TIME\": 3,\n",
    "        \"REQ_CPUS\": 4,\n",
    "        \"REQ_MEM\": 5,\n",
    "        \"REQ_NODES\": 6,\n",
    "        \"TIME_LIMIT_RAW\": 7,\n",
    "        \"PRIORITY\": 8\n",
    "    }\n",
    "\n",
    "    # Columns: job_id, jobs_running, cpus_running, memory_running, nodes_running, time_limit_raw\n",
    "    running_features = np.zeros((np_array.shape[0], 6))\n",
    "\n",
    "    num_trees = 50\n",
    "    tree_size = 100000\n",
    "    tree_overlap = 10000\n",
    "\n",
    "\n",
    "    # Creation of overlapping interval trees\n",
    "    print(\"Creating interval trees\")\n",
    "    count = 0\n",
    "    tree_idx = 0\n",
    "    trees = []\n",
    "    for i in range(num_trees):\n",
    "        trees.append(IntervalTree())\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        if np_array[job_idx, idx_dict[\"START_TIME\"]] < np_array[job_idx, idx_dict[\"END_TIME\"]]:\n",
    "            trees[tree_idx][np_array[job_idx, idx_dict[\"START_TIME\"]].astype(np.int32):np_array[job_idx, idx_dict[\"END_TIME\"]].astype(np.int32)] = job_idx\n",
    "        # Make last tree size of 3 normal trees to prevent edge case issues\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tmp = sorted(trees[tree_idx])[-tree_overlap:]\n",
    "            for interval in tmp:\n",
    "                trees[tree_idx + 1][interval[0]:interval[1]] = interval[2]\n",
    "            if tree_idx != 0:\n",
    "                tmp = sorted(trees[tree_idx])[0:tree_overlap]\n",
    "                for interval in tmp:\n",
    "                    trees[tree_idx - 1][interval[0]:interval[1]] = interval[2]\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "\n",
    "    print(\"Looping through jobs\")\n",
    "    # Loop through jobs and add in data for all jobs whose trees overlap\n",
    "    tree_idx = 0\n",
    "    count = 0\n",
    "    progress = 0\n",
    "    total = np_array.shape[0]\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        progress += 1\n",
    "        running_features[job_idx, 0] = np_array[job_idx, idx_dict[\"JOB_ID\"]]\n",
    "\n",
    "        for overlapping in trees[tree_idx][np_array[job_idx, idx_dict[\"ELIGIBLE\"]]]:\n",
    "            if overlapping[2] != np_array[job_idx, idx_dict[\"JOB_ID\"]]:\n",
    "                jobs_running_idx = overlapping[2]\n",
    "                running_features[job_idx, 1] += 1\n",
    "                running_features[job_idx, 2] += np_array[jobs_running_idx, idx_dict[\"REQ_CPUS\"]].astype(np.uint32)\n",
    "                running_features[job_idx, 3] += np_array[jobs_running_idx, idx_dict[\"REQ_MEM\"]]\n",
    "                running_features[job_idx, 4] += np_array[jobs_running_idx, idx_dict[\"REQ_NODES\"]].astype(np.uint32)\n",
    "                running_features[job_idx, 5] += np_array[jobs_running_idx, idx_dict[\"TIME_LIMIT_RAW\"]].astype(np.uint32)\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "\n",
    "    # Update dataframe from results and upload to database\n",
    "    new_df = pd.DataFrame(\n",
    "        {\"job_id\": running_features[:, 0].astype(np.uint32), \"jobs_running\": running_features[:, 1].astype(np.uint32),\n",
    "         \"cpus_running\": running_features[:, 2].astype(np.uint32),\n",
    "         \"memory_running\": running_features[:, 3], \"nodes_running\": running_features[:, 4].astype(np.uint32),\n",
    "         \"time_limit_running\": running_features[:, 5].astype(np.uint32)})\n",
    "    all_df.update(new_df)\n",
    "    all_df.to_sql('jobs', engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "def calculate_queue_features():\n",
    "    \"\"\"\n",
    "    calculate_queue_features()\n",
    "\n",
    "    Calculates features relating to jobs currently in the queue when another job\n",
    "    is made eligible. Features include number of jobs queued, the total number of\n",
    "    cpus in use by queued jobs, the total amount of memory being used by\n",
    "    queued jobs, the total amount of nodes being used by queued jobs, and the\n",
    "    combined timelimit for all queued jobs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    engine = read_db.create_engine()\n",
    "    # Read in dataframe\n",
    "    all_df = pd.read_sql_query(\"SELECT * FROM jobs_2021_2025_05_02 ORDER BY eligible\", engine)\n",
    "    df = pd.read_sql_query(\n",
    "        \"SELECT job_id, eligible, start_time, end_time, req_cpus, req_mem, req_nodes, time_limit_raw FROM jobs_2021_2025_05_02 ORDER BY eligible\",\n",
    "        engine)\n",
    "    df['start_time'] = df['start_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['end_time'] = df['end_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['eligible'] = df['eligible'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    np_array = df.to_numpy()\n",
    "\n",
    "    idx_dict = {\n",
    "        \"JOB_ID\": 0,\n",
    "        \"ELIGIBLE\": 1,\n",
    "        \"START_TIME\": 2,\n",
    "        \"END_TIME\": 3,\n",
    "        \"REQ_CPUS\": 4,\n",
    "        \"REQ_MEM\": 5,\n",
    "        \"REQ_NODES\": 6,\n",
    "        \"TIME_LIMIT_RAW\": 7,\n",
    "        \"PRIORITY\": 8\n",
    "    }\n",
    "    \n",
    "    engine.dispose()\n",
    "\n",
    "    # Columns: job_id, jobs_ahead_queue, cpus_ahead_queue, memory_ahead_queue, nodes_ahead_queue, time_limit_raw\n",
    "    queue_features = np.zeros((np_array.shape[0], 6))\n",
    "\n",
    "    num_trees = 50\n",
    "    tree_size = 100000\n",
    "    tree_overlap = 10000\n",
    "\n",
    "    # Creation of interval trees\n",
    "    count = 0\n",
    "    tree_idx = 0\n",
    "    trees = []\n",
    "    for i in range(num_trees):\n",
    "        trees.append(IntervalTree())\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        if np_array[job_idx, idx_dict[\"ELIGIBLE\"]] != np_array[job_idx, idx_dict[\"START_TIME\"]]:\n",
    "            trees[tree_idx][np_array[job_idx, idx_dict[\"ELIGIBLE\"]]:np_array[job_idx, idx_dict[\"START_TIME\"]]] = job_idx\n",
    "        # Make last tree size of 3 normal trees to prevent edge case issues\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tmp = sorted(trees[tree_idx])[-tree_overlap:]\n",
    "            for interval in tmp:\n",
    "                trees[tree_idx + 1][interval[0]:interval[1]] = interval[2]\n",
    "            if tree_idx != 0:\n",
    "                tmp = sorted(trees[tree_idx])[0:tree_overlap]\n",
    "                for interval in tmp:\n",
    "                    trees[tree_idx - 1][interval[0]:interval[1]] = interval[2]\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "\n",
    "    # Loop through jobs and add in data for all jobs whose trees overlap\n",
    "    tree_idx = 0\n",
    "    count = 0\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        queue_features[job_idx, 0] = np_array[job_idx, idx_dict[\"JOB_ID\"]]\n",
    "        for overlapping in trees[tree_idx][np_array[job_idx, idx_dict[\"ELIGIBLE\"]]]:\n",
    "            if overlapping[2] != np_array[job_idx, idx_dict[\"JOB_ID\"]]:\n",
    "                jobs_running_idx = overlapping[2]\n",
    "                queue_features[job_idx, 1] += 1\n",
    "                queue_features[job_idx, 2] += np_array[jobs_running_idx, idx_dict[\"REQ_CPUS\"]].astype(np.uint32)\n",
    "                queue_features[job_idx, 3] += np_array[jobs_running_idx, idx_dict[\"REQ_MEM\"]]\n",
    "                queue_features[job_idx, 4] += np_array[jobs_running_idx, idx_dict[\"REQ_NODES\"]].astype(np.uint32)\n",
    "                queue_features[job_idx, 5] += np_array[jobs_running_idx, idx_dict[\"TIME_LIMIT_RAW\"]].astype(np.uint32)\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "\n",
    "    # Update dataframe from results and upload to database\n",
    "    new_df = pd.DataFrame(\n",
    "        {\"job_id\": queue_features[:, 0].astype(np.uint32), \"jobs_ahead_queue\": queue_features[:, 1].astype(np.uint32),\n",
    "         \"cpus_ahead_queue\": queue_features[:, 2].astype(np.uint32),\n",
    "         \"memory_ahead_queue\": queue_features[:, 3], \"nodes_ahead_queue\": queue_features[:, 4].astype(np.uint32),\n",
    "         \"time_limit_ahead_queue\": queue_features[:, 5].astype(np.uint32)})\n",
    "    all_df.update(new_df)\n",
    "    all_df.to_sql('jobs_queue_only', engine, if_exists='replace', index=False)\n",
    "\n",
    "def calculate_higher_priority_queue_features():\n",
    "    \"\"\"\n",
    "    calculate_higher_priority_queue_features()\n",
    "\n",
    "    Calculates features relating to jobs currently in the queue when another job\n",
    "    is made eligible with a higher priority than said job. Features include \n",
    "    number of jobs queued, the total number of\n",
    "    cpus in use by queued jobs, the total amount of memory being used by\n",
    "    queued jobs, the total amount of nodes being used by queued jobs, and the\n",
    "    combined timelimit for queued jobs with a higher priority.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    engine = read_db.create_engine()\n",
    "    # Read in dataframe\n",
    "    print(\"Reading in dataframes\")\n",
    "    all_df = pd.read_sql_query(\"SELECT * FROM jobs_2021_2025_05_02 ORDER BY eligible \", engine)\n",
    "    df = pd.read_sql_query(\n",
    "        \"SELECT job_id, eligible, start_time, end_time, req_cpus, req_mem, req_nodes, time_limit_raw, priority FROM jobs_2021_2025_05_02 ORDER BY eligible\",\n",
    "        engine)\n",
    "    df['start_time'] = df['start_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['end_time'] = df['end_time'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    df['eligible'] = df['eligible'].apply(lambda x: x.timestamp()).astype('int64')\n",
    "    np_array = df.to_numpy()\n",
    "\n",
    "    print(\"Dataframes successfully read into numpy arrays\")\n",
    "    \n",
    "    idx_dict = {\n",
    "        \"JOB_ID\": 0,\n",
    "        \"ELIGIBLE\": 1,\n",
    "        \"START_TIME\": 2,\n",
    "        \"END_TIME\": 3,\n",
    "        \"REQ_CPUS\": 4,\n",
    "        \"REQ_MEM\": 5,\n",
    "        \"REQ_NODES\": 6,\n",
    "        \"TIME_LIMIT_RAW\": 7,\n",
    "        \"PRIORITY\": 8\n",
    "    }\n",
    "\n",
    "    # Columns: job_id, jobs_ahead_queue_priority, cpus_ahead_queue_priority, memory_ahead_queue_priority, nodes_ahead_queue_priority, time_limit_raw_queue_priority\n",
    "    queue_features = np.zeros((np_array.shape[0], 11))\n",
    "    engine.dispose()\n",
    "    num_trees = 50\n",
    "    tree_size = 100000\n",
    "    tree_overlap = 10000\n",
    "\n",
    "    # Creation of interval trees\n",
    "    count = 0\n",
    "    tree_idx = 0\n",
    "    trees = []\n",
    "            \n",
    "            \n",
    "    for i in range(num_trees):\n",
    "        trees.append(IntervalTree())\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        if np_array[job_idx, idx_dict[\"ELIGIBLE\"]] != np_array[job_idx, idx_dict[\"START_TIME\"]]:\n",
    "            trees[tree_idx][np_array[job_idx, idx_dict[\"ELIGIBLE\"]]:np_array[job_idx, idx_dict[\"START_TIME\"]]] = job_idx\n",
    "        # Make last tree size of 3 normal trees to prevent edge case issues\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tmp = sorted(trees[tree_idx])[-tree_overlap:]\n",
    "            for interval in tmp:\n",
    "                trees[tree_idx + 1][interval[0]:interval[1]] = interval[2]\n",
    "            if tree_idx != 0:\n",
    "                tmp = sorted(trees[tree_idx])[0:tree_overlap]\n",
    "                for interval in tmp:\n",
    "                    trees[tree_idx - 1][interval[0]:interval[1]] = interval[2]\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "\n",
    "    # Loop through jobs and add in data for all jobs whose trees overlap\n",
    "    tree_idx = 0\n",
    "    count = 0\n",
    "    progress = 0\n",
    "    total = np_array.shape[0]\n",
    "    for job_idx in tqdm(range(np_array.shape[0])):\n",
    "        count += 1\n",
    "        queue_features[job_idx, 0] = np_array[job_idx, idx_dict[\"JOB_ID\"]]\n",
    "        progress += 1\n",
    "        clear_output(wait=True)\n",
    "        to_print = '#' * (round(100 * progress / total, -1))\n",
    "        print(to_print, f\"...{count} / {progress} ... {count / progress}\")\n",
    "        for overlapping in trees[tree_idx][np_array[job_idx, idx_dict[\"ELIGIBLE\"]]]:\n",
    "            if overlapping[2] != np_array[job_idx, idx_dict[\"JOB_ID\"]]:\n",
    "                jobs_running_idx = overlapping[2]\n",
    "                # If job's priority is greater than current job's priority\n",
    "                # Used to calculate features only for jobs with higher priority\n",
    "                queue_features[job_idx, 1] += 1\n",
    "                queue_features[job_idx, 2] += np_array[jobs_running_idx, idx_dict[\"REQ_CPUS\"]].astype(np.uint32)\n",
    "                queue_features[job_idx, 3] += np_array[jobs_running_idx, idx_dict[\"REQ_MEM\"]]\n",
    "                queue_features[job_idx, 4] += np_array[jobs_running_idx, idx_dict[\"REQ_NODES\"]].astype(np.uint32)\n",
    "                queue_features[job_idx, 5] += np_array[jobs_running_idx, idx_dict[\"TIME_LIMIT_RAW\"]].astype(np.uint32)\n",
    "                \n",
    "                if np_array[jobs_running_idx, idx_dict[\"PRIORITY\"]] > np_array[job_idx, idx_dict[\"PRIORITY\"]]:\n",
    "                    queue_features[job_idx, 6] += 1\n",
    "                    queue_features[job_idx, 7] += np_array[jobs_running_idx, idx_dict[\"REQ_CPUS\"]].astype(np.uint32)\n",
    "                    queue_features[job_idx, 8] += np_array[jobs_running_idx, idx_dict[\"REQ_MEM\"]]\n",
    "                    queue_features[job_idx, 9] += np_array[jobs_running_idx, idx_dict[\"REQ_NODES\"]].astype(np.uint32)\n",
    "                    queue_features[job_idx, 10] += np_array[jobs_running_idx, idx_dict[\"TIME_LIMIT_RAW\"]].astype(np.uint32)\n",
    "        if count == tree_size and ((np_array.shape[0] - job_idx) > (3 * tree_size)):\n",
    "            tree_idx += 1\n",
    "            count = 0\n",
    "    \n",
    "    engine = read_db.create_engine()\n",
    "\n",
    "    # Update dataframe from results and upload to database\n",
    "    new_df = pd.DataFrame(\n",
    "        {\"job_id\": queue_features[:, 0].astype(np.uint32),\n",
    "         \"jobs_ahead_queue\": queue_features[:, 1].astype(np.uint32),\n",
    "         \"cpus_ahead_queue\": queue_features[:, 2].astype(np.uint32),\n",
    "         \"memory_ahead_queue\": queue_features[:, 3].astype(np.uint32),\n",
    "         \"nodes_ahead_queue\": queue_features[:, 4].astype(np.uint32),\n",
    "         \"time_limit_ahead_queue\": queue_features[:, 5].astype(np.uint32),\n",
    "         \"jobs_ahead_queue_priority\": queue_features[:, 1+5].astype(np.uint32),\n",
    "         \"cpus_ahead_queue_priority\": queue_features[:, 2+5].astype(np.uint32),\n",
    "         \"memory_ahead_queue_priority\": queue_features[:, 3+5], \"nodes_ahead_queue_priority\": queue_features[:, 4+5].astype(np.uint32),\n",
    "         \"time_limit_ahead_queue_priority\": queue_features[:, 5+5].astype(np.uint32)})\n",
    "    all_df.update(new_df)\n",
    "    all_df.to_sql('jobs_queue_and_priority', engine, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    engine = read_db.create_engine()\n",
    "    # calculate_running_features(engine)\n",
    "    # calculate_queue_features(engine)\n",
    "    calculate_higher_priority_queue_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655d452-3b2b-4f2e-8eed-879469314717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)\n",
   "language": "python",
   "name": "anaconda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
